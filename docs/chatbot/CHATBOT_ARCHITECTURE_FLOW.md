# ğŸ—ï¸ Chatbot Architecture & Flow Diagram

**Date**: 2026-02-12 00:30:00 EST
**Purpose**: Visual understanding of how the chatbot works from browser to AI response

---

## ğŸŒ COMPLETE USER FLOW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          USER BROWSER                                      â”‚
â”‚                   (Chrome/Safari/Firefox/Edge)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ 1. User navigates to
                                      â”‚    cuttingedge.cihconsultingllc.com
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MAIN WEBSITE                                           â”‚
â”‚                   cutting-edge_barber-shop_1                                â”‚
â”‚                      (Docker Container)                                     â”‚
â”‚                  Port: 80 (external)                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ 2. Page loads, shows "Need Help" button
                                      â”‚    (FloatingConcierge.tsx component)
                                      â”‚
                                      â”‚ 3. User clicks button
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MODAL POPUP                                            â”‚
â”‚                   Shows two options:                                        â”‚
â”‚                   - Voice Mode â†’ https://voice-ce.cihconsultingllc.com      â”‚
â”‚                   - Chat Mode â†’ https://chat.cuttingedge.cihconsultingllc.comâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ 4. User clicks "Chat Mode"
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BROWSER NAVIGATION                                      â”‚
â”‚              https://chat.cuttingedge.cihconsultingllc.com                   â”‚
â”‚                      (Nginx reverse proxy)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ 5. Nginx routes to internal container
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CHATBOT CONTAINER                                         â”‚
â”‚                  cutting-edge_chatbot_1                                      â”‚
â”‚                      (Docker Container)                                     â”‚
â”‚                  Port: 3001 (internal) â†’ 80 (external)                      â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  React Frontend (ChatInterface.tsx)                                â”‚   â”‚
â”‚  â”‚  - Renders chat UI                                                  â”‚   â”‚
â”‚  â”‚  - Handles user input                                               â”‚   â”‚
â”‚  â”‚  - Displays messages                                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â”‚  Environment Variable: VITE_API_URL=/api                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ 6. User types "Hello" and clicks Send
                                      â”‚    Frontend makes HTTP POST request
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   NGINX REVERSE PROXY                                       â”‚
â”‚                  (Routes /api/* to handoff-api)                              â”‚
â”‚                                                                              â”‚
â”‚  Request: POST /api/chat                                                    â”‚
â”‚  Body: {"message":"Hello"}                                                   â”‚
â”‚  Headers: Content-Type: application/json                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ 7. Nginx proxies to internal container
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   HANDOFF-API CONTAINER                                      â”‚
â”‚                  cutting-edge-handoff-api                                     â”‚
â”‚                      (Docker Container)                                      â”‚
â”‚                  Port: 3000 (internal)                                      â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Hono Framework (index.ts)                                          â”‚   â”‚
â”‚  â”‚  - Receives POST /api/chat request                                  â”‚   â”‚
â”‚  â”‚  - Validates input                                                   â”‚   â”‚
â”‚  â”‚  - Calls chatService.ts                                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                         â”‚
â”‚                                    â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  chatService.ts (Business Logic)                                     â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Step 1: Check Ollama connection status                             â”‚   â”‚
â”‚  â”‚  Step 2: Search knowledge base (PostgreSQL + pgvector)              â”‚   â”‚
â”‚  â”‚  Step 3: Get relevant context from database                          â”‚   â”‚
â”‚  â”‚  Step 4: Construct prompt with context                              â”‚   â”‚
â”‚  â”‚  Step 5: Call Ollama API for LLM generation                         â”‚   â”‚
â”‚  â”‚  Step 6: Format response and return                                  â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Retry Logic: let lastError (FIXED - was const before)             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                         â”‚
â”‚                                    â”‚ 8. Need AI knowledge                     â”‚
â”‚                                    â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PostgreSQL Database (supabase-db)                                   â”‚   â”‚
â”‚  â”‚  - knowledge_base table                                              â”‚   â”‚
â”‚  â”‚  - pgvector extension                                                â”‚   â”‚
â”‚  â”‚  - Vector similarity search                                          â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  SELECT * FROM search_knowledge_base('user query', 5);                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                         â”‚
â”‚                                    â”‚ 9. Need LLM generation                  â”‚
â”‚                                    â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Docker DNS Resolution                                              â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  handoff-api container can reach:                                    â”‚   â”‚
â”‚  â”‚  - ollama (fabricaio_fabricaio_net)                                 â”‚   â”‚
â”‚  â”‚  - supabase-db (bridge network)                                     â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Docker network: multi-homed (connected to 3 networks)               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ 10. HTTP POST to Ollama
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OLLAMA CONTAINER                                      â”‚
â”‚                      (Separate Docker container)                            â”‚
â”‚                                                                              â”‚
â”‚  Model: llama3.2 (or latest)                                                â”‚
â”‚  Embeddings: nomic-embed-text                                              â”‚
â”‚  API Port: 11434                                                            â”‚
â”‚                                                                              â”‚
â”‚  Process:                                                                    â”‚
â”‚  1. Receive prompt with context                                             â”‚
â”‚  2. Run LLM inference (takes 15-25 seconds)                                â”‚
â”‚  3. Return generated text                                                   â”‚
â”‚                                                                              â”‚
â”‚  Note: Ollama runs on fabricaio_fabricaio_net network                       â”‚
â”‚        but handoff-api can reach it via Docker DNS                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ 11. Return AI response
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   HANDOFF-API CONTAINER (Return Path)                        â”‚
â”‚                                                                              â”‚
â”‚  chatService.ts:                                                             â”‚
â”‚  - Receives LLM response from Ollama                                        â”‚
â”‚  - Formats as JSON: {response: "AI text here"}                              â”‚
â”‚  - Returns HTTP 200 with response body                                      â”‚
â”‚                                                                              â”‚
â”‚  Retry Logic: If Ollama fails, retry up to 3 times (using let lastError)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ 12. HTTP 200 Response
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   NGINX REVERSE PROXY (Return Path)                         â”‚
â”‚                                                                              â”‚
â”‚  Adds CORS headers:                                                          â”‚
â”‚  - Access-Control-Allow-Origin: *                                           â”‚
â”‚  - Access-Control-Allow-Methods: POST, GET, OPTIONS                         â”‚
â”‚  - Access-Control-Allow-Headers: Content-Type                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ 13. HTTP 200 Response
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BROWSER (ChatInterface.tsx)                                â”‚
â”‚                                                                              â”‚
â”‚  React Component:                                                            â”‚
â”‚  - Receives response via fetch() API                                       â”‚
â”‚  - Parses JSON: response.response                                           â”‚
â”‚  - Updates UI state with AI message                                         â”‚
â”‚  - Shows message in chat bubble                                             â”‚
â”‚  - Auto-scrolls to bottom                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ 14. User sees response
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER SEES:                                             â”‚
â”‚                                                                              â”‚
â”‚  Chat: Hello                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                                â”‚
â”‚  AI: Hello! I'm happy to assist you at Cutting Edge Barbershop...          â”‚
â”‚                                                                              â”‚
â”‚  Total time: ~20-30 seconds (most time is Ollama LLM generation)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ CRITICAL COMPONENTS

### 1. Frontend (Chatbot Container)
**File**: `ChatInterface.tsx`
**Location**: `cutting-edge_chatbot_1` container
**Port**: 3001 (internal) â†’ 80 (external via nginx)

**Key Code**:
```typescript
const response = await fetch('/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ message: userInput })
});
```

**Environment Variable** (CRITICAL - FIXED YESTERDAY):
```bash
# .env.production (CORRECT)
VITE_API_URL=/api

# WRONG (this was the bug!)
# VITE_API_URL=http://localhost:3000
```

### 2. Backend API (Handoff-API Container)
**File**: `chatService.ts`
**Location**: `cutting-edge-handoff-api` container
**Port**: 3000 (internal)

**Key Code** (FIXED YESTERDAY):
```typescript
// WRONG (this was the bug!)
const lastError: Error | null = null;
for (const attempt of retries) {
  try {
    return await callOllama(prompt);
  } catch (error) {
    lastError = error; // ERROR: Can't assign to const!
  }
}

// CORRECT (fixed)
let lastError: Error | null = null;
for (const attempt of retries) {
  try {
    return await callOllama(prompt);
  } catch (error) {
    lastError = error; // OK: Can assign to let
  }
}
```

**Flow**:
1. Receive message from frontend
2. Search knowledge base (PostgreSQL)
3. Get relevant context
4. Call Ollama LLM
5. Return response

### 3. Ollama (AI Generation)
**Container**: Separate (on fabricaio_fabricaio_net)
**Port**: 11434
**Models**:
- llama3.2 (main LLM)
- nomic-embed-text (embeddings)

**API Call**:
```bash
curl http://ollama:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2",
    "prompt": "Hello, please help with...",
    "stream": false
  }'
```

**Response Time**: 15-25 seconds (normal for local LLM)

### 4. PostgreSQL (Knowledge Base)
**Container**: supabase-db
**Port**: 5432
**Database**: postgres
**Tables**:
- knowledge_base (documents, embeddings)
- chat_logs (conversation history)

**Vector Search**:
```sql
SELECT * FROM search_knowledge_base('user query', 5);
-- Returns top 5 relevant documents using pgvector
```

---

## ğŸ³ DOCKER NETWORK ARCHITECTURE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCKER NETWORKS                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Network 1: cutting-edge_default (bridge)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ - cutting-edge-handoff-api (multi-homed)     â”‚
â”‚ - cutting-edge_chatbot_1                      â”‚
â”‚ - supabase-db                                 â”‚
â”‚ - cutting-edge_barber-shop_1                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Network 2: fabricaio_fabricaio_net (bridge)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ - ollama                                      â”‚
â”‚ - cutting-edge-handoff-api (multi-homed)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Network 3: Another network (bridge)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ - cutting-edge-handoff-api (multi-homed)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KEY INSIGHT: handoff-api is connected to ALL 3 networks
This allows it to reach:
- supabase-db (network 1)
- ollama (network 2)
- Other services (network 3)
```

**Docker DNS Resolution**:
```bash
# From handoff-api container, can reach:
curl http://ollama:11434/api/tags  # Works!
psql -h supabase-db -U postgres    # Works!
```

---

## ğŸ”’ SECURITY & CORS

### CORS Configuration (nginx)
```nginx
location /api/ {
    proxy_pass http://localhost:3000/;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;

    # CORS headers
    add_header Access-Control-Allow-Origin *;
    add_header Access-Control-Allow-Methods "POST, GET, OPTIONS";
    add_header Access-Control-Allow-Headers "Content-Type";
}
```

### Why This Matters:
- Frontend on https://chat.cuttingedge.cihconsultingllc.com
- API on https://chat.cuttingedge.cihconsultingllc.com/api/*
- Same domain = No CORS issues!
- But nginx still adds headers for safety

---

## âš¡ PERFORMANCE TIMELINE

```
User Action                | Time  | Component
--------------------------|-------|----------------------------------
Click "Need Help"         | 0ms   | FloatingConcierge.tsx
Modal appears             | 50ms  | React state update
Click "Chat Mode"         | 100ms | Navigation start
Navigate to chat URL      | 500ms | Browser + nginx routing
Chat interface loads      | 1.5s  | React app mount
Type "Hello"              | 2s    | User input time
Click Send                | 2.1s  | Form submission
POST to /api/chat         | 2.2s  | fetch() call
Backend receives request  | 2.3s  | Hono router
Search knowledge base     | 2.5s  | PostgreSQL vector search
Call Ollama LLM           | 3s    | HTTP request to Ollama
LLM inference processing  | 25s   | Ollama generates response
Receive LLM response      | 28s   | Backend gets result
Format & return           | 28.5s | chatService.ts
HTTP 200 response        | 29s   | nginx adds headers
Browser receives response | 29.5s | fetch() resolves
Update UI with response  | 30s   | React state update
User sees AI message     | 30s   | Render complete

TOTAL: ~30 seconds (normal for local LLM)
```

---

## ğŸ› BUGS FOUND & FIXED (2026-02-11)

### Bug #1: Backend const reassignment
**Location**: `chatService.ts:161`
**Code**:
```typescript
// BEFORE (BROKEN)
const lastError: Error | null = null;
for (const attempt of retries) {
  try { return await callOllama(); }
  catch (error) { lastError = error; } // ERROR!
}

// AFTER (FIXED)
let lastError: Error | null = null;
for (const attempt of retries) {
  try { return await callOllama(); }
  catch (error) { lastError = error; } // OK!
}
```

### Bug #2: Frontend localhost reference
**Location**: `.env.production`
**Code**:
```bash
# BEFORE (BROKEN)
VITE_API_URL=http://localhost:3000

# AFTER (FIXED)
VITE_API_URL=/api
```

### Bug #3: Misleading error message
**Frontend showed**: "LLM Connection failed. Is Ollama running?"
**Actual error**: "Assignment to constant variable"
**Result**: Confusing debugging!

---

## ğŸ“Š HEALTH CHECK ENDPOINTS

### 1. Chatbot Health Check
```bash
curl https://chat.cuttingedge.cihconsultingllc.com/api/health

# Expected Response:
{
  "status": "ok",
  "service": "cutting-edge-handoff-api",
  "timestamp": "2026-02-12T00:30:00Z",
  "uptime": 12345
}
```

### 2. Container Status
```bash
ssh contabo-vps "docker ps | grep cutting-edge"

# Expected Output:
cutting-edge-handoff-api   Up 3 minutes
cutting-edge_chatbot_1      Up 45 minutes
cutting-edge_barber-shop_1  Up 4 hours
```

### 3. Ollama Status
```bash
ssh contabo-vps "curl http://localhost:11434/api/tags"

# Expected Response:
{
  "models": [
    {"name": "llama3.2", ...},
    {"name": "nomic-embed-text", ...}
  ]
}
```

---

## ğŸ¯ SUCCESS CRITERIA

Chatbot is WORKING if:
- âœ… Navigate to chat URL without errors
- âœ… Chat interface renders properly
- âœ… Send message and get response within 30s
- âœ… No console errors in browser
- âœ… No 500 errors in Network tab
- âœ… AI responses are relevant

Chatbot is FAILING if:
- âŒ Browser crashes
- âŒ White screen or spinner forever
- âŒ "LLM Connection failed" error
- âŒ Messages don't send
- âŒ No response after 60 seconds
- âŒ Console shows red errors

---

**Last Updated**: 2026-02-12 00:30:00 EST
**Status**: All bugs fixed, system operational
**Next**: Browser-based validation testing needed
